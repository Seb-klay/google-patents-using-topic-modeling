{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7At0L9Xpc4S"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKaya8IhqS93"
      },
      "source": [
        "## Goal of this project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKe8E26-piJK"
      },
      "source": [
        "This project aims to answer one question : In the wake of third-party cookie deprecation, does Google implement alternative user tracking or targeting practices that may raise privacy disclosed to the public ? To answer this question, an hypothesis have yet to be answered :\n",
        "\n",
        "-\tH₀ (Null Hypothesis): The number of Google’s privacy-related patents filed after the announcement of the third-party cookie phase-out does not reflect ongoing development of alternative targeting systems.\n",
        "-\tH₁ (Alternative Hypothesis): The number of Google’s privacy-related patents filed after the announcement of the third-party cookie phase-out reflects ongoing development of alternative targeting systems.\n",
        "\n",
        "To answer this questions, the Google patents, from year 1995 to 2024, so for the past 29 years, are going to be explored. We are going to dig into them using topic modeling techniques. Creating clusters of the different patents and identify those that are related to tracking topics.\n",
        "\n",
        "We are going to answer the question by validating hypothesis such as H₀ or H₁. The significance level choosen is α = 0.05, which indicates a 5% risk of rejecting the null hypothesis when it is true. A statistical test (chi squared test of independence) will be used to compare the number of patents filed before and after the announcement. Finally, we will compare the p-value with α in order to validate or reject our hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grc_h4XPAis_"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gmk90bCutbQ"
      },
      "outputs": [],
      "source": [
        "# general purpose\n",
        "! pip install bertopic==0.17.0\n",
        "! pip install transformers==4.52.4\n",
        "! pip install sentence-transformers==4.1.0\n",
        "! pip install datamapplot==0.5.1\n",
        "! pip install pandas==2.2.2\n",
        "\n",
        "# data\n",
        "from google.colab import drive, files\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 2000)\n",
        "import seaborn as sns\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "# clustering\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "sentence_model = SentenceTransformer(\"AI-Growth-Lab/PatentSBERTa\") #all-MiniLM-L6-v2\n",
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Fine-tune your topic representations\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from bertopic.representation import MaximalMarginalRelevance\n",
        "\n",
        "# graphs & plots\n",
        "import plotly.express as px\n",
        "import datamapplot\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.colors import TwoSlopeNorm\n",
        "\n",
        "# Distance computing\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Classification\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# Hypothesis validation\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Tests\n",
        "from itertools import combinations\n",
        "# AJOUTER test_rbo.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wS3nSnxG8nC"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umBZ-hb1gEg7"
      },
      "source": [
        "For this project, we're taking 38'752 patents from Google. The purpose is to use a ML model to classify these patents and analyse the work Google made the past few years regarding privacy and tracking. More than 100'000 were gathered but a lot of duplicates were present in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-DYnAO5v8of"
      },
      "source": [
        "## Data source"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BSHl9qLv-6a"
      },
      "source": [
        "For this work, the data were collected on Google Patents using BigQuery retrieving Google past patents until December 2024.\n",
        "\n",
        "The following query was used to retrieve the patents :\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "    p.publication_number,\n",
        "    (SELECT text FROM UNNEST(p.title_localized) WHERE language = 'en' LIMIT 1) AS title,\n",
        "    p.filing_date,\n",
        "    p.publication_date,\n",
        "    p.grant_date,\n",
        "    a.name AS assignee_name,\n",
        "    (SELECT text FROM UNNEST(p.abstract_localized) WHERE language = 'en' LIMIT 1) AS abstract\n",
        "FROM\n",
        "    `patents-public-data.patents.publications` AS p\n",
        "LEFT JOIN\n",
        "    UNNEST(p.assignee_harmonized) AS a\n",
        "WHERE\n",
        "    LOWER(a.name) LIKE '%google%'\n",
        "ORDER BY\n",
        "    p.publication_date DESC\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1STXnFiZYdP"
      },
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBgckTR9Gnow"
      },
      "outputs": [],
      "source": [
        "# Import data with Drive or install Drive with collab interface\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM8fuZBEGv6Q"
      },
      "outputs": [],
      "source": [
        "# Create panda dataset\n",
        "patents = pd.read_excel('/content/drive/MyDrive/bq-results-20250211-170449-1739293514946.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kS_pVU0ZcUE"
      },
      "source": [
        "## Quick analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dntJtOkNC8f"
      },
      "outputs": [],
      "source": [
        "# visualization\n",
        "patents.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieLdm2aRNHOD"
      },
      "outputs": [],
      "source": [
        "# Checking size\n",
        "patents.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdM7qdxyNi56"
      },
      "outputs": [],
      "source": [
        "patents.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3r8EIY0Zevb"
      },
      "outputs": [],
      "source": [
        "# Number of NaN and Null\n",
        "patents.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjtXT4yJbuI1"
      },
      "source": [
        "We can see on this table that 4'802 titles and 15'635 abstracts are missing.\n",
        "\n",
        "If it is only one out of the two, we can deal with that. But if both are missing, it is useless to keep these lines as long as the purpose is to categorize text on titles, abstracts or both.\n",
        "\n",
        "We're going to check if both of the lines are missing. If that's the case, we can just delete the line because having no information at all is not usefull for the text classification if we have no text at all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWzJdn7Nc0bS"
      },
      "outputs": [],
      "source": [
        "# Count rows where both are null\n",
        "both_null = patents[(patents[\"title\"].isnull()) & (patents[\"abstract\"].isnull())]\n",
        "len(both_null)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYRCNlk3eFW0"
      },
      "source": [
        "After checking both columns, we're not going to delete all of the 15'635 lines because we only have 4'668 lines that are completely null.\n",
        "\n",
        "Now, let's make sure that all of the patents are from Google, part of Alphabet Inc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsvCaFIH63Bl"
      },
      "outputs": [],
      "source": [
        "patents['assignee_name'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a2BhC2H9rQX"
      },
      "outputs": [],
      "source": [
        "companies_not_related = [\n",
        "    'GOOGLE LIFE SCIENCES LLC', # Now called Verily and divest Alphabet Inc. industry but not related to our topics\n",
        "    'PEARL HAI GOOGLE ELECTRONIC CO LTD', # Chinese company with Google in its name\n",
        "    'SHENZHEN GOOGLE WEIXUN TECH CO LTD', # Other chinese company having Google in its name and not being related\n",
        "    'JURONG GOOGLE MANOR MODERN AGRICULTURAL TECHNOLOGY DEV CO LTD', # Other chinese company having Google in its name\n",
        "    'GOOGLE SWEDEN TECH AB', # A swedish company\n",
        "    'GOOGLE SWEDEN TECHNIQUE AB', # The same swedish company\n",
        "    'REAL ESTATE GOOGLE CO LTD' # Google Real Estate is part of Alphabet Inc. but not related to our topics\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGi60lxIGLWR"
      },
      "source": [
        "We are now going to check for duplicates. Since we have more than 100'000 patents, we want to make sure that all of them are unique,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSy46jm_GTBM"
      },
      "outputs": [],
      "source": [
        "patents.duplicated(subset='abstract').sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LipnSg-CIpdQ"
      },
      "source": [
        "This shows that 68835 line has a duplicate. We are going to count all duplicates and delete them later in the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av2887LKHgNp"
      },
      "outputs": [],
      "source": [
        "print(len(patents['abstract'].unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmDDXqndZ09w"
      },
      "source": [
        "## Data treatment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH-haVRYe4g3"
      },
      "outputs": [],
      "source": [
        "# Delete lines that are empty in both title and abstract columns\n",
        "patents_nn = patents.drop(both_null.index)\n",
        "\n",
        "# Replace null lines with \" \"\n",
        "patents_nn[\"title\"] = patents_nn[\"title\"].fillna(\"\")\n",
        "patents_nn[\"abstract\"] = patents_nn[\"abstract\"].fillna(\"\")\n",
        "\n",
        "len(patents_nn) # 102'935"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ_hjrjDerH7"
      },
      "outputs": [],
      "source": [
        "# Format date columns\n",
        "patents_dformated = patents_nn.copy(deep=True)\n",
        "\n",
        "patents_dformated['filing_date'] = pd.to_datetime(patents_nn[\"filing_date\"].astype(str), format='%Y%m%d')\n",
        "patents_dformated['publication_date'] = pd.to_datetime(patents_nn[\"publication_date\"].astype(str), format='%Y%m%d')\n",
        "patents_dformated['grant_date'] = pd.to_datetime(patents_nn[\"grant_date\"].astype(str), format='%Y%m%d', errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ5U2jznf7eu"
      },
      "outputs": [],
      "source": [
        "# Delete duplicates\n",
        "patents_dformated = patents_dformated.drop_duplicates(subset='abstract').reset_index(drop=True)\n",
        "\n",
        "len(patents_dformated) # 38768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-Ho5xsfeyEz"
      },
      "outputs": [],
      "source": [
        "# Adding a mix column used for pre-processing later\n",
        "patents_dformated['processing'] = patents_dformated[\"title\"] + \" \" + patents_dformated[\"abstract\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksegYGxpD9oN"
      },
      "outputs": [],
      "source": [
        "# Delete some unwanted characters\n",
        "unwanted_chars = [\"&#39;\", \"&#34;\", \"&amp;\", \"-\", \"/\", \"\\\\\", \"“\", \"”\", \"~\"]\n",
        "\n",
        "for char in unwanted_chars:\n",
        "    patents_dformated['processing'] = patents_dformated['processing'].str.replace(char, '', regex=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFiFOypdB2pL"
      },
      "outputs": [],
      "source": [
        "# Delete the unwanted companies\n",
        "patents_dformated = patents_dformated[~patents_dformated['assignee_name'].isin(companies_not_related)] # 38752"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afIR6GipqeTq"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGWuUawNqt0g"
      },
      "source": [
        "Here come the clustering techniques. For this project, the main tool used is BERTopic.This tool allows us to extract coherent topics from textual data by combining embeddings with clustering algorithms and generating interpretable topics. However, BERTopic is not working alone but with several dependencies.\n",
        "\n",
        "UMAP is a dimensionality reduction technique that preserves the structure of high-dimensional embeddings while making them easier to visualize and cluster.\n",
        "\n",
        "HDBSCAN is a density-based clustering algorithm that identifies groups of similar documents and filters out noise or unrelated data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTmn63eJubCG"
      },
      "source": [
        "### Training of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rD7eL_1I-vQ"
      },
      "outputs": [],
      "source": [
        "# Create a copy\n",
        "processing = patents_dformated[['processing']].copy(deep=True)\n",
        "\n",
        "# Put it in a list\n",
        "processing_list = processing.iloc[:, 0].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFn2EavW18ZK"
      },
      "outputs": [],
      "source": [
        "# # Prepare embeddings\n",
        "# embeddings = sentence_model.encode(processing_list, show_progress_bar=True, batch_size=64)\n",
        "\n",
        "# # Save embeddings to a .npy file\n",
        "# np.save('/content/drive/My Drive/embeddings_patents_google.npy', embeddings)\n",
        "\n",
        "# Load embeddings from .npy file\n",
        "embeddings = np.load('/content/drive/My Drive/embeddings_patents_google.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hebUjfrm3SFH"
      },
      "outputs": [],
      "source": [
        "# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
        "umap_model = UMAP(\n",
        "    n_neighbors=100,\n",
        "    n_components=3,\n",
        "    min_dist=0.5,\n",
        "    metric='cosine',\n",
        "    random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3Zp2MFtrQZE"
      },
      "source": [
        "UMAP is used to reduce the dimensions of the embeddings created by BERT. For the need of seeing the global clusters of the dataset, we focused on bigger parameters.\n",
        "\n",
        "100 n_neighbors has been choosen for that purpose. n_components of 3 is the final dimension reduction to make it more suitable for clustering and visualization. min_dist of 0.5 has been choosen because it is in the middle of 0 and 1 for this parameter, and choosing a bigger one makes the clusters too big and less understandable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egnXhDuNfO9Z"
      },
      "outputs": [],
      "source": [
        "# Added because advised to control number of topics through the cluster model (hdbscan by default)\n",
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size=100,\n",
        "    max_cluster_size=5000,\n",
        "    metric='euclidean',\n",
        "    cluster_selection_method='eom')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3juiygYrq5Y"
      },
      "source": [
        "HDBSCAN is used for clustering. Because we need to see the global clusters, we had to put a minimum size on the cluster of 100 and a maximum size of 5000 (even though this is overlaps when doing the dimensional reduction with BERTopic)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rusIwGcPl2y3"
      },
      "outputs": [],
      "source": [
        "# Removing the stop-words because BERTopic does not do it by default\n",
        "vectorizer_model = CountVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1, 3), # Extract unigrams (1-word), bigrams (2-word), and trigrams (3-word phrases) from the text\n",
        "    min_df=10 # Ignore terms that appear in less than 10 documents\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0OB7oaT3zk-"
      },
      "outputs": [],
      "source": [
        "# Use KeyBERTInspired and MaximalMarginalRelevance for our representation model to (1) keep useful words and (2) produce cleaner topic words\n",
        "representation_model=[KeyBERTInspired(top_n_words=30), MaximalMarginalRelevance(diversity=.5)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jet7y1kI7Bm_"
      },
      "outputs": [],
      "source": [
        "# # Train BERTopic\n",
        "# topic_model = BERTopic(\n",
        "#     embedding_model=sentence_model,\n",
        "#     umap_model=umap_model,\n",
        "#     hdbscan_model=hdbscan_model,\n",
        "#     vectorizer_model=vectorizer_model,\n",
        "#     representation_model=representation_model\n",
        "#     )\n",
        "\n",
        "# topics, probs = topic_model.fit_transform(\n",
        "#     processing_list,\n",
        "#     embeddings\n",
        "#     )\n",
        "# topic_model.save(\"/content/drive/My Drive/google_patents_model_cosine_bis\")\n",
        "\n",
        "# Or load BERTopic in BERTopic v0.9.2 or higher:\n",
        "topic_model = BERTopic.load(\"/content/drive/My Drive/google_patents_model_cosine_bis\")\n",
        "topics = topic_model._map_predictions(topic_model.hdbscan_model.labels_)\n",
        "probs = topic_model.hdbscan_model.probabilities_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1URj0fP2z_71"
      },
      "outputs": [],
      "source": [
        "# Let's see the information given, the amount of topics per cluster, the type of groups we have, etc.\n",
        "topic_model.get_topic_info()[['Topic', 'Count', 'Name', 'Representation']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4Ucl7ftzd7H"
      },
      "source": [
        "We have, however, too many outliers. We want to reduce them before keeping on going"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-M-Q5pTzmX0"
      },
      "outputs": [],
      "source": [
        "# Reduce outliers using the `embeddings` strategy\n",
        "reduced_topics = topic_model.reduce_outliers(\n",
        "    processing_list,\n",
        "    topics,\n",
        "    strategy=\"embeddings\",\n",
        "    embeddings=embeddings,\n",
        "    threshold=0.5 # The threshold for assigning topics to outlier documents\n",
        "    )\n",
        "\n",
        "# Update topics\n",
        "topic_model.update_topics(\n",
        "    processing_list,\n",
        "    topics=reduced_topics,\n",
        "    vectorizer_model=vectorizer_model\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQgqWGK50NCW"
      },
      "outputs": [],
      "source": [
        "# Let's check a second time\n",
        "topic_model.get_topic_info()[['Topic', 'Count', 'Name', 'Representation']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHQ7itwXlRqG"
      },
      "source": [
        "## Assessing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phjuieMylURc"
      },
      "source": [
        "Before we keep on going, it would be interesting to assess our model to see if it was trained properly and have a good confidence with the following results. For that, we will..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1UeM1egdYNL"
      },
      "source": [
        "List of good topics evaluations : https://github.com/jonaschn/awesome-topic-models?tab=readme-ov-file#models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3_Ls5mMbQy7"
      },
      "source": [
        "### Topic model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ORMwpn9aJjN"
      },
      "source": [
        "\"There are two main aspects to evaluate topic models:\n",
        "- coherence\n",
        "- relevance.\n",
        "\n",
        "Coherence measures how well the words in a topic are related to each other, based on their semantic similarity or frequency.\n",
        "Relevance measures how well the topics capture the main themes or aspects of the documents, based on their importance r specificity.\n",
        "There are various metrics and tools to calculate coherence and relevance, such as C_V, U_Mass, topic coherence pipeline, etc. You can also use human judgment or feedback to assess the interpretability and usefulness of your topics.\"\n",
        "\n",
        "This LinkedIn post, \"How do you evaluate the quality and relevance of your topic models and clusters ?\", tells us a bit about methods to accomplish this task. We'll first focus on the topic model and then on the clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1X5hspk09lNM"
      },
      "outputs": [],
      "source": [
        "# Get list of words that are used for the topic modeling assessment\n",
        "topics_to_evaluate = topic_model.get_topic_info()['Representation']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsIg0QW8Z-qP"
      },
      "source": [
        "#### Coherence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coherence assessment should be executed with a tool like OCTIS. However, some versioning problems were encountered. Thus, it has been decided to keep only the 3 other metrics."
      ],
      "metadata": {
        "id": "3HP3g_afX_le"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A8FCmdybXcP"
      },
      "source": [
        "#### Relevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0DKr_8kbZ-B"
      },
      "outputs": [],
      "source": [
        "# How many of the top-N words in each topic are unique across topics (non-overlapping).\n",
        "def proportion_unique_words(topics, topk=10):\n",
        "    \"\"\"\n",
        "    compute the proportion of unique words\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    topics: a list of lists of words\n",
        "    topk: top k words on which the topic diversity will be computed\n",
        "    \"\"\"\n",
        "    if topk > len(topics[0]):\n",
        "        raise Exception('Words in topics are less than '+str(topk))\n",
        "    else:\n",
        "        unique_words = set()\n",
        "        for topic in topics:\n",
        "            unique_words = unique_words.union(set(topic[:topk]))\n",
        "        puw = len(unique_words) / (topk * len(topics))\n",
        "        return puw\n",
        "\n",
        "#Result 1.0 would mean that all words are unique across topics.\n",
        "proportion_unique_words(topics_to_evaluate, topk=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFb4kgFYZYgh"
      },
      "source": [
        "A proportion_unique_words score of ~0.73 means that around 27% of the top topic words are reused across topics, suggesting moderate redundancy. This means that reducing topics would be an option in this case to reduce redundancy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8xgWm1wbZdt"
      },
      "source": [
        "### Clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4cVKrzcbjA9"
      },
      "source": [
        "\"There are two main aspects to evaluate clusters :\n",
        "- validity\n",
        "- stability\n",
        "\n",
        "Validity measures how well the clusters reflect the true structure or similarity of the data, based on their compactness, separation, or silhouette.\n",
        "Stability measures how consistent the clusters are across different runs or samples of the data, based on their bobustness, sensitivity, or agreement. There are various metrics and tools to calculate validity and stability, such as Davies-Bouldin index, Rand index, cluster validation toolbox, etc.\n",
        "\n",
        "You can also use domain knowledge or business goals to assess the relevance and value of your clusters.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa6AaezF7QjP"
      },
      "source": [
        "#### Validity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Wldia9TbcLl"
      },
      "outputs": [],
      "source": [
        "# Jaccard similarity across top-N words for each pair of topics.\n",
        "def pairwise_jaccard_diversity(topics, topk=10):\n",
        "    '''\n",
        "    compute the average pairwise jaccard distance between the topics\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    topics: a list of lists of words\n",
        "    topk: top k words on which the topic diversity\n",
        "          will be computed\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pjd: average pairwise jaccard distance\n",
        "    '''\n",
        "    dist = 0\n",
        "    count = 0\n",
        "    for list1, list2 in combinations(topics, 2):\n",
        "        js = 1 - len(set(list1).intersection(set(list2)))/len(set(list1).union(set(list2)))\n",
        "        dist = dist + js\n",
        "        count = count + 1\n",
        "    return dist/count\n",
        "\n",
        "# Result 1.0 would mean that there are no shared words between any topic pairs\n",
        "pairwise_jaccard_diversity(topics_to_evaluate, topk=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVkpt2FCZ4WB"
      },
      "source": [
        "A validity score of 0.962 indicates that the topics are quite distinctive and well-separated. There would be no need to reduce them according to this measure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw0Xmb807j8f"
      },
      "source": [
        "#### Stability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MuxXgYoY78_"
      },
      "source": [
        "The following code has been hard coded because of some troubles during installation of the file. It directly comes from https://github.com/silviatti/topic-model-diversity/blob/master/rbo.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyZHEIUwIIQc"
      },
      "outputs": [],
      "source": [
        "\"\"\"Rank-biased overlap, a ragged sorted list similarity measure.\n",
        "See http://doi.acm.org/10.1145/1852102.1852106 for details. All functions\n",
        "directly corresponding to concepts from the paper are named so that they can be\n",
        "clearly cross-identified.\n",
        "The definition of overlap has been modified to account for ties. Without this,\n",
        "results for lists with tied items were being inflated. The modification itself\n",
        "is not mentioned in the paper but seems to be reasonable, see function\n",
        "``overlap()``. Places in the code which diverge from the spec in the paper\n",
        "because of this are highlighted with comments.\n",
        "The two main functions for performing an RBO analysis are ``rbo()`` and\n",
        "``rbo_dict()``; see their respective docstrings for how to use them.\n",
        "The following doctest just checks that equivalent specifications of a\n",
        "problem yield the same result using both functions:\n",
        "    >>> lst1 = [{\"c\", \"a\"}, \"b\", \"d\"]\n",
        "    >>> lst2 = [\"a\", {\"c\", \"b\"}, \"d\"]\n",
        "    >>> ans_rbo = _round(rbo(lst1, lst2, p=.9))\n",
        "    >>> dct1 = dict(a=1, b=2, c=1, d=3)\n",
        "    >>> dct2 = dict(a=1, b=2, c=2, d=3)\n",
        "    >>> ans_rbo_dict = _round(rbo_dict(dct1, dct2, p=.9, sort_ascending=True))\n",
        "    >>> ans_rbo == ans_rbo_dict\n",
        "    True\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import division\n",
        "\n",
        "import math\n",
        "from bisect import bisect_left\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "RBO = namedtuple(\"RBO\", \"min res ext\")\n",
        "RBO.__doc__ += \": Result of full RBO analysis\"\n",
        "RBO.min.__doc__ = \"Lower bound estimate\"\n",
        "RBO.res.__doc__ = \"Residual corresponding to min; min + res is an upper bound estimate\"\n",
        "RBO.ext.__doc__ = \"Extrapolated point estimate\"\n",
        "\n",
        "\n",
        "def _round(obj):\n",
        "    if isinstance(obj, RBO):\n",
        "        return RBO(_round(obj.min), _round(obj.res), _round(obj.ext))\n",
        "    else:\n",
        "        return round(obj, 3)\n",
        "\n",
        "\n",
        "def set_at_depth(lst, depth):\n",
        "    ans = set()\n",
        "    for v in lst[:depth]:\n",
        "        if isinstance(v, set):\n",
        "            ans.update(v)\n",
        "        else:\n",
        "            ans.add(v)\n",
        "    return ans\n",
        "\n",
        "\n",
        "def raw_overlap(list1, list2, depth):\n",
        "    \"\"\"Overlap as defined in the article.\n",
        "    \"\"\"\n",
        "    set1, set2 = set_at_depth(list1, depth), set_at_depth(list2, depth)\n",
        "    return len(set1.intersection(set2)), len(set1), len(set2)\n",
        "\n",
        "\n",
        "def overlap(list1, list2, depth):\n",
        "    \"\"\"Overlap which accounts for possible ties.\n",
        "    This isn't mentioned in the paper but should be used in the ``rbo*()``\n",
        "    functions below, otherwise overlap at a given depth might be > depth which\n",
        "    inflates the result.\n",
        "    There are no guidelines in the paper as to what's a good way to calculate\n",
        "    this, but a good guess is agreement scaled by the minimum between the\n",
        "    requested depth and the lengths of the considered lists (overlap shouldn't\n",
        "    be larger than the number of ranks in the shorter list, otherwise results\n",
        "    are conspicuously wrong when the lists are of unequal lengths -- rbo_ext is\n",
        "    not between rbo_min and rbo_min + rbo_res.\n",
        "    >>> overlap(\"abcd\", \"abcd\", 3)\n",
        "    3.0\n",
        "    >>> overlap(\"abcd\", \"abcd\", 5)\n",
        "    4.0\n",
        "    >>> overlap([\"a\", {\"b\", \"c\"}, \"d\"], [\"a\", {\"b\", \"c\"}, \"d\"], 2)\n",
        "    2.0\n",
        "    >>> overlap([\"a\", {\"b\", \"c\"}, \"d\"], [\"a\", {\"b\", \"c\"}, \"d\"], 3)\n",
        "    3.0\n",
        "    \"\"\"\n",
        "    ov = agreement(list1, list2, depth) * min(depth, len(list1), len(list2))\n",
        "    return ov\n",
        "    # NOTE: comment the preceding and uncomment the following line if you want\n",
        "    # to stick to the algorithm as defined by the paper\n",
        "    # return raw_overlap(list1, list2, depth)[0]\n",
        "\n",
        "\n",
        "def agreement(list1, list2, depth):\n",
        "    \"\"\"Proportion of shared values between two sorted lists at given depth.\n",
        "    >>> _round(agreement(\"abcde\", \"abdcf\", 1))\n",
        "    1.0\n",
        "    >>> _round(agreement(\"abcde\", \"abdcf\", 3))\n",
        "    0.667\n",
        "    >>> _round(agreement(\"abcde\", \"abdcf\", 4))\n",
        "    1.0\n",
        "    >>> _round(agreement(\"abcde\", \"abdcf\", 5))\n",
        "    0.8\n",
        "    >>> _round(agreement([{1, 2}, 3], [1, {2, 3}], 1))\n",
        "    0.667\n",
        "    >>> _round(agreement([{1, 2}, 3], [1, {2, 3}], 2))\n",
        "    1.0\n",
        "    \"\"\"\n",
        "    len_intersection, len_set1, len_set2 = raw_overlap(list1, list2, depth)\n",
        "    return 2 * len_intersection / (len_set1 + len_set2)\n",
        "\n",
        "\n",
        "def cumulative_agreement(list1, list2, depth):\n",
        "    return (agreement(list1, list2, d) for d in range(1, depth + 1))\n",
        "\n",
        "\n",
        "def average_overlap(list1, list2, depth=None):\n",
        "    \"\"\"Calculate average overlap between ``list1`` and ``list2``.\n",
        "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 1))\n",
        "    0.0\n",
        "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 2))\n",
        "    0.0\n",
        "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 3))\n",
        "    0.222\n",
        "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 4))\n",
        "    0.292\n",
        "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 5))\n",
        "    0.313\n",
        "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 6))\n",
        "    0.317\n",
        "    >>> _round(average_overlap(\"abcdefg\", \"zcavwxy\", 7))\n",
        "    0.312\n",
        "    \"\"\"\n",
        "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
        "    return sum(cumulative_agreement(list1, list2, depth)) / depth\n",
        "\n",
        "\n",
        "def rbo_at_k(list1, list2, p, depth=None):\n",
        "    # ``p**d`` here instead of ``p**(d - 1)`` because enumerate starts at\n",
        "    # 0\n",
        "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
        "    d_a = enumerate(cumulative_agreement(list1, list2, depth))\n",
        "    return (1 - p) * sum(p ** d * a for (d, a) in d_a)\n",
        "\n",
        "\n",
        "def rbo_min(list1, list2, p, depth=None):\n",
        "    \"\"\"Tight lower bound on RBO.\n",
        "    See equation (11) in paper.\n",
        "    >>> _round(rbo_min(\"abcdefg\", \"abcdefg\", .9))\n",
        "    0.767\n",
        "    >>> _round(rbo_min(\"abcdefgh\", \"abcdefg\", .9))\n",
        "    0.767\n",
        "    \"\"\"\n",
        "    depth = min(len(list1), len(list2)) if depth is None else depth\n",
        "    x_k = overlap(list1, list2, depth)\n",
        "    log_term = x_k * math.log(1 - p)\n",
        "    sum_term = sum(\n",
        "        p ** d / d * (overlap(list1, list2, d) - x_k) for d in range(1, depth + 1)\n",
        "    )\n",
        "    return (1 - p) / p * (sum_term - log_term)\n",
        "\n",
        "\n",
        "def rbo_res(list1, list2, p):\n",
        "    \"\"\"Upper bound on residual overlap beyond evaluated depth.\n",
        "    See equation (30) in paper.\n",
        "    NOTE: The doctests weren't verified against manual computations but seem\n",
        "    plausible. In particular, for identical lists, ``rbo_min()`` and\n",
        "    ``rbo_res()`` should add up to 1, which is the case.\n",
        "    >>> _round(rbo_res(\"abcdefg\", \"abcdefg\", .9))\n",
        "    0.233\n",
        "    >>> _round(rbo_res(\"abcdefg\", \"abcdefghijklmnopqrstuvwxyz\", .9))\n",
        "    0.239\n",
        "    \"\"\"\n",
        "    S, L = sorted((list1, list2), key=len)\n",
        "    s, l = len(S), len(L)\n",
        "    x_l = overlap(list1, list2, l)\n",
        "    # since overlap(...) can be fractional in the general case of ties and f\n",
        "    # must be an integer --> math.ceil()\n",
        "    f = int(math.ceil(l + s - x_l))\n",
        "    # upper bound of range() is non-inclusive, therefore + 1 is needed\n",
        "    term1 = s * sum(p ** d / d for d in range(s + 1, f + 1))\n",
        "    term2 = l * sum(p ** d / d for d in range(l + 1, f + 1))\n",
        "    term3 = x_l * (math.log(1 / (1 - p)) - sum(p ** d / d for d in range(1, f + 1)))\n",
        "    return p ** s + p ** l - p ** f - (1 - p) / p * (term1 + term2 + term3)\n",
        "\n",
        "\n",
        "def rbo_ext(list1, list2, p):\n",
        "    \"\"\"RBO point estimate based on extrapolating observed overlap.\n",
        "    See equation (32) in paper.\n",
        "    NOTE: The doctests weren't verified against manual computations but seem\n",
        "    plausible.\n",
        "    >>> _round(rbo_ext(\"abcdefg\", \"abcdefg\", .9))\n",
        "    1.0\n",
        "    >>> _round(rbo_ext(\"abcdefg\", \"bacdefg\", .9))\n",
        "    0.9\n",
        "    \"\"\"\n",
        "    S, L = sorted((list1, list2), key=len)\n",
        "    s, l = len(S), len(L)\n",
        "    x_l = overlap(list1, list2, l)\n",
        "    x_s = overlap(list1, list2, s)\n",
        "    # the paper says overlap(..., d) / d, but it should be replaced by\n",
        "    # agreement(..., d) defined as per equation (28) so that ties are handled\n",
        "    # properly (otherwise values > 1 will be returned)\n",
        "    # sum1 = sum(p**d * overlap(list1, list2, d)[0] / d for d in range(1, l + 1))\n",
        "    sum1 = sum(p ** d * agreement(list1, list2, d) for d in range(1, l + 1))\n",
        "    sum2 = sum(p ** d * x_s * (d - s) / s / d for d in range(s + 1, l + 1))\n",
        "    term1 = (1 - p) / p * (sum1 + sum2)\n",
        "    term2 = p ** l * ((x_l - x_s) / l + x_s / s)\n",
        "    return term1 + term2\n",
        "\n",
        "\n",
        "def rbo(list1, list2, p):\n",
        "    \"\"\"Complete RBO analysis (lower bound, residual, point estimate).\n",
        "    ``list`` arguments should be already correctly sorted iterables and each\n",
        "    item should either be an atomic value or a set of values tied for that\n",
        "    rank. ``p`` is the probability of looking for overlap at rank k + 1 after\n",
        "    having examined rank k.\n",
        "    >>> lst1 = [{\"c\", \"a\"}, \"b\", \"d\"]\n",
        "    >>> lst2 = [\"a\", {\"c\", \"b\"}, \"d\"]\n",
        "    >>> _round(rbo(lst1, lst2, p=.9))\n",
        "    RBO(min=0.489, res=0.477, ext=0.967)\n",
        "    \"\"\"\n",
        "    if not 0 <= p <= 1:\n",
        "        raise ValueError(\"The ``p`` parameter must be between 0 and 1.\")\n",
        "    args = (list1, list2, p)\n",
        "    return RBO(rbo_min(*args), rbo_res(*args), rbo_ext(*args))\n",
        "\n",
        "\n",
        "def sort_dict(dct, *, ascending=False):\n",
        "    \"\"\"Sort keys in ``dct`` according to their corresponding values.\n",
        "    Sorts in descending order by default, because the values are\n",
        "    typically scores, i.e. the higher the better. Specify\n",
        "    ``ascending=True`` if the values are ranks, or some sort of score\n",
        "    where lower values are better.\n",
        "    Ties are handled by creating sets of tied keys at the given position\n",
        "    in the sorted list.\n",
        "    >>> dct = dict(a=1, b=2, c=1, d=3)\n",
        "    >>> list(sort_dict(dct)) == ['d', 'b', {'a', 'c'}]\n",
        "    True\n",
        "    >>> list(sort_dict(dct, ascending=True)) == [{'a', 'c'}, 'b', 'd']\n",
        "    True\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    items = []\n",
        "    # items should be unique, scores don't have to\n",
        "    for item, score in dct.items():\n",
        "        if not ascending:\n",
        "            score *= -1\n",
        "        i = bisect_left(scores, score)\n",
        "        if i == len(scores):\n",
        "            scores.append(score)\n",
        "            items.append(item)\n",
        "        elif scores[i] == score:\n",
        "            existing_item = items[i]\n",
        "            if isinstance(existing_item, set):\n",
        "                existing_item.add(item)\n",
        "            else:\n",
        "                items[i] = {existing_item, item}\n",
        "        else:\n",
        "            scores.insert(i, score)\n",
        "            items.insert(i, item)\n",
        "    return items\n",
        "\n",
        "\n",
        "def rbo_dict(dict1, dict2, p, *, sort_ascending=False):\n",
        "    \"\"\"Wrapper around ``rbo()`` for dict input.\n",
        "    Each dict maps items to be sorted to the score according to which\n",
        "    they should be sorted. The RBO analysis is then performed on the\n",
        "    resulting sorted lists.\n",
        "    The sort is descending by default, because scores are typically the\n",
        "    higher the better, but this can be overridden by specifying\n",
        "    ``sort_ascending=True``.\n",
        "    >>> dct1 = dict(a=1, b=2, c=1, d=3)\n",
        "    >>> dct2 = dict(a=1, b=2, c=2, d=3)\n",
        "    >>> _round(rbo_dict(dct1, dct2, p=.9, sort_ascending=True))\n",
        "    RBO(min=0.489, res=0.477, ext=0.967)\n",
        "    \"\"\"\n",
        "    list1, list2 = (\n",
        "        sort_dict(dict1, ascending=sort_ascending),\n",
        "        sort_dict(dict2, ascending=sort_ascending),\n",
        "    )\n",
        "    return rbo(list1, list2, p)\n",
        "\n",
        "\n",
        "if __name__ in (\"__main__\", \"__console__\"):\n",
        "    import doctest\n",
        "\n",
        "    doctest.testmod()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl2OkUVG7mmY"
      },
      "outputs": [],
      "source": [
        "# A ranking-aware similarity metric between two ranked lists (usually top-N words in topics).\n",
        "def irbo(topics, weight=0.9, topk=10):\n",
        "    \"\"\"\n",
        "    compute the inverted rank-biased overlap\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    topics: a list of lists of words\n",
        "    weight: p (float), default 1.0: Weight of each\n",
        "        agreement at depth d:p**(d-1). When set\n",
        "        to 1.0, there is no weight, the rbo returns\n",
        "        to average overlap.\n",
        "    topk: top k words on which the topic diversity\n",
        "          will be computed\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    irbo : score of the rank biased overlap over the topics\n",
        "    \"\"\"\n",
        "    if topk > len(topics[0]):\n",
        "        raise Exception('Words in topics are less than topk')\n",
        "    else:\n",
        "        collect = []\n",
        "        for list1, list2 in combinations(topics, 2):\n",
        "            word2index = get_word2index(list1, list2)\n",
        "            indexed_list1 = [word2index[word] for word in list1]\n",
        "            indexed_list2 = [word2index[word] for word in list2]\n",
        "            rbo_val = rbo(indexed_list1[:topk], indexed_list2[:topk], p=weight)[2]\n",
        "            collect.append(rbo_val)\n",
        "        return 1 - np.mean(collect)\n",
        "\n",
        "def get_word2index(list1, list2):\n",
        "    words = set(list1)\n",
        "    words = words.union(set(list2))\n",
        "    word2index = {w: i for i, w in enumerate(words)}\n",
        "    return word2index\n",
        "\n",
        "# Result 1.0 would mean that there is no overlap even considering rank — perfect diversity.\n",
        "print(\"irbo p=0.5:\",irbo(topics_to_evaluate, weight=0.5, topk=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg6xdQLvaHfr"
      },
      "source": [
        "An IRBO score of 0.99 (Inverted Rank-Biased Overlap) means the topics are highly diverse — their top words barely overlap in ranked order. This is another measure saying that the topics are well separated.\n",
        "\n",
        "However, Bertopics allows other ways to measure how similar clusters are. We can use them to give a final opinion on our topic modeling and keep on going."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjyhAPmKuAb4"
      },
      "source": [
        "#### Final thoughts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vd3i_mOZyDg_"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_heatmap()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H64gdq5AyEm8"
      },
      "source": [
        "On the heatmap, we confirm that we have to reduce a few clusters similar between each other. We can proceed to reduce them before keeping on going.\n",
        "\n",
        "Before that, let's also have a look at the distance of each topics. This allows us to see the biggest topics but also their distance to each other and see if it is well spreaded over or all aggregated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot4_TZaIz5Bz"
      },
      "outputs": [],
      "source": [
        "# show how topics overlaps each other and the need to merge them\n",
        "topic_model.visualize_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HhflZXUsZf2"
      },
      "source": [
        "This graph shows that a few clusters have a close distance to each other.  It often means they're discussing very similar themes, with minor variations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0vGcFqZb9FM"
      },
      "outputs": [],
      "source": [
        "# Further reduce topics\n",
        "topic_model.reduce_topics(processing_list, nr_topics=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8M_RyCVcSuP"
      },
      "source": [
        "Let's visualize the heatmap one more time and we are good to go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZavR0sfcW7e"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_heatmap()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYoN2DcwaC3J"
      },
      "source": [
        "# Clustering with BERTopics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcA4NfCPuPuO"
      },
      "source": [
        "## Most important topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5On6rX-yuUBL"
      },
      "source": [
        "We can now see the biggest topics and what they are talking about. Our goal is to see the main focus of Google R&D over the years, especially with tracking and profiling techniques. This will allow us to see if advertisement topics are part of the main ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMB3Z2MJymnc"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_barchart(top_n_topics=15, n_words=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ2WMYFo1Elc"
      },
      "source": [
        "Now, we can see the documents all together and how they are clustered. This gives us a representation of the documents and the topics they are linked to in the blick of an eye.\n",
        "\n",
        "By doing this, we can see the amount of importance the advertisement topics (or related topics) are in the whole dataset and dig into that later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2Py_YH5vkjW"
      },
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dTXnfyXZJAC"
      },
      "source": [
        "Now, let's take a look at the documents all together. By visualizing the documents and aggregate them in topics, we can see the overall structure of all of the topics.\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gsk8xq0_j9l"
      },
      "outputs": [],
      "source": [
        "umap_settings = [\n",
        "    {\"n_neighbors\": 15, \"min_dist\": 0.99},\n",
        "    {\"n_neighbors\": 200, \"min_dist\": 0.1},\n",
        "    {\"n_neighbors\": 15, \"min_dist\": 0.1},\n",
        "    {\"n_neighbors\": 200, \"min_dist\": 0.99}\n",
        "]\n",
        "\n",
        "# Reduce dimensionnality and train every parameters\n",
        "visualization_figures = []\n",
        "for param in umap_settings:\n",
        "    # Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
        "    reduced_embeddings = UMAP(\n",
        "        n_neighbors=param[\"n_neighbors\"],\n",
        "        min_dist=param[\"min_dist\"],\n",
        "        n_components=2,\n",
        "        metric='cosine',\n",
        "        random_state=42\n",
        "        ).fit_transform(embeddings)\n",
        "\n",
        "    # Train every parameters\n",
        "    fig_param = topic_model.visualize_documents(\n",
        "        processing_list,\n",
        "        reduced_embeddings=reduced_embeddings,\n",
        "        title=\"Patent clusters from Google\"\n",
        "        )\n",
        "    visualization_figures.append(fig_param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUS4aVdlLUgy"
      },
      "outputs": [],
      "source": [
        "visualization_figures[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zae8cCvgLXMU"
      },
      "outputs": [],
      "source": [
        "visualization_figures[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDwjJzzczO-p"
      },
      "outputs": [],
      "source": [
        "visualization_figures[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXYFFx1GzOK9"
      },
      "outputs": [],
      "source": [
        "visualization_figures[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuqZkm5QuozM"
      },
      "source": [
        "We can also generate another type of maps in order to zoom in and out more freely ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsfaO6lY9bF8"
      },
      "outputs": [],
      "source": [
        "# Process the topic model to visualize the documents\n",
        "fig_with_datamapplot = topic_model.visualize_document_datamap(\n",
        "    processing_list,\n",
        "    reduced_embeddings=reduced_embeddings,\n",
        "    interactive=True)\n",
        "\n",
        "# Save the second visualization with datamapplot\n",
        "fig_with_datamapplot.save(\"/content/drive/My Drive/patents_from_google_with_datamapplot.html\")\n",
        "\n",
        "# Show plot\n",
        "fig_with_datamapplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3tO-FzzS83l"
      },
      "source": [
        "What this maps shows is the global clusters of the Google patents since the beginning to end of 2024. We can see a few main focus over advertisement and search query (for ads and search engine), audio speach voice (which should be related to assistant like Google home), neural network (for the need of AI products since a the beginning of the decade), etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SdFwpaZvbjM"
      },
      "source": [
        "## Overall evolution of the patents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLPTQ7aswqx0"
      },
      "source": [
        "We are now going to see the evolution of the overall patents. This will give us an idea of the proportions of the topic in Google's focus.\n",
        "\n",
        "Google is primarly a company that sells ads. However, it is also known to have other businesses running like AI assistant with Google Home, Self-driving cars, Streaming plateforms, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsdaJOGi1T35"
      },
      "outputs": [],
      "source": [
        "# get all topics (int) and add them to every documents\n",
        "patents_dformated[\"topics\"] = topic_model.topics_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkY1FSm4wi-q"
      },
      "source": [
        "### Overall evolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR0cajJOOSAd"
      },
      "outputs": [],
      "source": [
        "# Add publication_year\n",
        "patents_dformated[\"publication_year\"] = patents_dformated[\"publication_date\"].dt.year\n",
        "\n",
        "# Group by year and topic\n",
        "count_df = patents_dformated.groupby(\n",
        "    [\"publication_year\", \"topics\"]\n",
        ").size().reset_index(name=\"count\")\n",
        "\n",
        "# Plot with color = topics\n",
        "plot_topic_by_year = px.bar(\n",
        "    count_df,\n",
        "    x=\"publication_year\",\n",
        "    y=\"count\",\n",
        "    color=\"topics\",\n",
        "    title=\"Number of patents per year by Topic\",\n",
        "    barmode=\"stack\"\n",
        ")\n",
        "\n",
        "# Save as HTML\n",
        "plot_topic_by_year.write_html(\"/content/drive/My Drive/patents_by_topic_chart.html\")\n",
        "\n",
        "# Show the chart\n",
        "plot_topic_by_year.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4vls8ymOnIs"
      },
      "outputs": [],
      "source": [
        "# Get the topic list to get an idea of the bar chart on top\n",
        "topic_model.get_topic_info()[['Topic', 'Count', 'Name', 'Representation']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVZ064JgJ94R"
      },
      "source": [
        "### Topics over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raJsfWYG75K6"
      },
      "outputs": [],
      "source": [
        "# Process ad-topics related over time\n",
        "topics_over_time = topic_model.topics_over_time(\n",
        "    patents_dformated.iloc[:, 0].tolist(),\n",
        "    patents_dformated['publication_date'].tolist(),\n",
        "    nr_bins=20\n",
        "    )\n",
        "\n",
        "# Save the plot\n",
        "topics_over_time.to_html(\"/content/drive/My Drive/topics_over_time.html\")\n",
        "\n",
        "# Show ad-topics related over time\n",
        "topic_model.visualize_topics_over_time(\n",
        "    topics_over_time,\n",
        "    topics=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] # do not show topic 0 because we dig into it later\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bda-IEbbRFhn"
      },
      "source": [
        "This graph helps us better see which topic grew the most over time. Interesting to note some pics in 2014-2016 with search query, ads, geographic location, social graphs, etc. Some others like image display, audio speech voice data, sation base wireless and neural networks are growing for a few years."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJaqxnqNdROL"
      },
      "source": [
        "## Digging inside the clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9Ara_q71w-I"
      },
      "source": [
        "We first want to get our privacy and tracking topics. Using this as an anchor, we can calculate the difference between these topics and every other topics. Then, we can check which are the most related to privacy and surveillance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yu99WXz-6if"
      },
      "source": [
        "Let's find where, if there are any, are located the topics for surveillance and privacy. We can then use them to compute our cosine distance between documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jrxx5X-GrKQ9"
      },
      "outputs": [],
      "source": [
        "similar_topics_surveillance, similarity_surveillance = topic_model.find_topics(\"tracking\", top_n=1)\n",
        "similar_topics_privacy, similarity_privacy = topic_model.find_topics(\"data privacy\", top_n=1)\n",
        "\n",
        "print(f'surveillance topic : {similar_topics_surveillance} %:{similarity_surveillance[0]}')\n",
        "print(f'privacy topic : {similar_topics_privacy} %:{similarity_privacy[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyMgpoin-dfA"
      },
      "source": [
        "For the following part, the idea was to calculate the cosine similarity of opposite documents or topics like \"privacy\" and \"tracking\".\n",
        "\n",
        "The result is that models based on the distributional hypothesis are not very good at this because antonyms often appear in similar contexts. So if we calculate the cosine similarity of opposite words like the ones we suggested, we would arrive at more or less the same result when using, let's say, a correlation bar chart.\n",
        "\n",
        "For that reason, we are going to compute the polarity score between a privacy topic (positive) and a tracking topic (negative). Let's first get these topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doc7iC0l5h85"
      },
      "source": [
        "### Visualization of the subtopics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5rWVyPmgasm"
      },
      "source": [
        "Our goal is to answer one question. We'd like to check if Google implement alternative user tracking or targeting practices. For this purpose, we have our patents list that we modeled by topics. We want now to dig in the topics that are related the most to these practices.\n",
        "\n",
        "According to BERTopic find_topics() method, they are located in one big aggregated cluster (0). Let's dig into this topic and see if they fit our definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2heAWwXii-o"
      },
      "source": [
        "#### Model parametering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nany4KPLkgJf"
      },
      "outputs": [],
      "source": [
        "# Removing the stop-words because BERTopic does not do it by default\n",
        "vectorizer_submodel_indiv = CountVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1, 3) # Extract unigrams (1-word), bigrams (2-word), and trigrams (3-word phrases) from the text\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gbmN51y8uGL"
      },
      "outputs": [],
      "source": [
        "# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
        "umap_submodel = UMAP(\n",
        "    n_neighbors=30,\n",
        "    n_components=3,\n",
        "    min_dist=0.1,\n",
        "    metric='cosine',\n",
        "    random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE1t1tvC26yi"
      },
      "outputs": [],
      "source": [
        "# Added because advised to control number of topics through the cluster model (hdbscan by default)\n",
        "hdbscan_submodel = HDBSCAN(\n",
        "    min_cluster_size=20,\n",
        "    max_cluster_size=500,\n",
        "    metric='euclidean',\n",
        "    cluster_selection_method='leaf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zw4JWBxIvHPO"
      },
      "outputs": [],
      "source": [
        "# Fet all embeddings (float) and add them to the dataframe\n",
        "patents_dformated[\"embeddings\"] = [vec.tolist() for vec in embeddings]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtDl2NnE7Zyy"
      },
      "source": [
        "#### Privacy and Tracking topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61INqUjDp6Yh"
      },
      "outputs": [],
      "source": [
        "sub_topics = []\n",
        "sub_topics.extend(topic_model.find_topics('privacy', top_n=1)[0])\n",
        "sub_topics.extend(topic_model.find_topics('tracking', top_n=1)[0])\n",
        "# Delete duplicates\n",
        "sub_topics = list(set(sub_topics))\n",
        "\n",
        "# Filter the emdeddings just on the topic mentionned above\n",
        "# (reprocessing the embeddings again would take several hours)\n",
        "filtered_embeddings_sub_topics_df = patents_dformated.loc[patents_dformated[\"topics\"].isin(sub_topics)]\n",
        "filtered_sub_topics_df = filtered_embeddings_sub_topics_df['processing']\n",
        "# convert the embeddings\n",
        "filtered_sub_embeddings = filtered_embeddings_sub_topics_df[\"embeddings\"].tolist()\n",
        "filtered_sub_embeddings_mat = csr_matrix(filtered_sub_embeddings)\n",
        "filtered_sub_embeddings = filtered_sub_embeddings_mat.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzQVdqGB-t3s"
      },
      "outputs": [],
      "source": [
        "# Train the new subtopics model\n",
        "# subtopic_model = BERTopic(\n",
        "#     embedding_model=sentence_model,\n",
        "#     umap_model=umap_submodel,\n",
        "#     hdbscan_model=hdbscan_submodel,\n",
        "#     vectorizer_model=vectorizer_submodel_indiv,\n",
        "#     representation_model=representation_model\n",
        "#     )\n",
        "\n",
        "# sub_model_topics, sub_model_probs = subtopic_model.fit_transform(filtered_sub_topics_df, filtered_sub_embeddings)\n",
        "# subtopic_model.save(\"/content/drive/My Drive/google_patents_model_subtopics_bis\")\n",
        "\n",
        "# Or load BERTopic in BERTopic v0.9.2 or higher:\n",
        "subtopic_model = BERTopic.load(\"/content/drive/My Drive/google_patents_model_subtopics_bis\")\n",
        "sub_model_topics = subtopic_model._map_predictions(subtopic_model.hdbscan_model.labels_)\n",
        "sub_model_probs = subtopic_model.hdbscan_model.probabilities_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHcsCBJi6s0m"
      },
      "outputs": [],
      "source": [
        "# Reduce outliers using the `embeddings` strategy\n",
        "reduced_subtopics = subtopic_model.reduce_outliers(\n",
        "    documents=filtered_sub_topics_df.to_list(),\n",
        "    topics=sub_model_topics,\n",
        "    strategy=\"embeddings\",\n",
        "    embeddings=filtered_sub_embeddings,\n",
        "    threshold=0.80\n",
        "    )\n",
        "\n",
        "# Update topics\n",
        "subtopic_model.update_topics(\n",
        "    filtered_sub_topics_df.to_list(),\n",
        "    topics=reduced_subtopics,\n",
        "    vectorizer_model=vectorizer_submodel_indiv\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y80_HOau_Itl"
      },
      "outputs": [],
      "source": [
        "reduced_filtered_submodel_embeddings = UMAP(\n",
        "    n_neighbors=15,\n",
        "    min_dist=0.1,\n",
        "    n_components=2,\n",
        "    metric='cosine',\n",
        "    random_state=42\n",
        "    ).fit_transform(filtered_sub_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9i9N6fo_cFi"
      },
      "outputs": [],
      "source": [
        "# Put it in the new document visualisation\n",
        "# Process the subtopic model to visualize the documents\n",
        "fig_with_datamapplot_subtopics = subtopic_model.visualize_document_datamap(\n",
        "    docs=filtered_sub_topics_df.tolist(),\n",
        "    reduced_embeddings=reduced_filtered_submodel_embeddings,\n",
        "    interactive=True\n",
        "    )\n",
        "\n",
        "# Save the second visualization with datamapplot\n",
        "fig_with_datamapplot_subtopics.save(\"/content/drive/My Drive/patents_from_google_with_datamapplot_subtopics_bis.html\")\n",
        "\n",
        "# Show plot\n",
        "fig_with_datamapplot_subtopics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqcChQXyY3p3"
      },
      "source": [
        "Let's make an assessment of our model before going further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-qQ1LD6ZIim"
      },
      "outputs": [],
      "source": [
        "# Get list of words that are used for the topic modeling assessment\n",
        "subtopics_to_evaluate = subtopic_model.get_topic_info()['Representation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWlkVBHJZmPn"
      },
      "outputs": [],
      "source": [
        "# Result 1.0 would mean that all words are unique across topics.\n",
        "print(\"proportion of unique words, topk=10:\",proportion_unique_words(subtopics_to_evaluate, topk=10))\n",
        "\n",
        "# Result 1.0 would mean that there are no shared words between any topic pairs\n",
        "print(\"pairwise jaccard diversity, topk=10:\",pairwise_jaccard_diversity(subtopics_to_evaluate, topk=10))\n",
        "\n",
        "# Result 1.0 would mean that there is no overlap even considering rank — perfect diversity.\n",
        "print(\"irbo, p=0.5:\",irbo(subtopics_to_evaluate, weight=0.5, topk=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DVnK0RDMyns"
      },
      "source": [
        "Before moving on, let's take a look at the evolution of the topics over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zse1BYneM2JZ"
      },
      "outputs": [],
      "source": [
        "# Process ad-topics related over time\n",
        "subtopics_over_time = subtopic_model.topics_over_time(\n",
        "    filtered_sub_topics_df.tolist(),\n",
        "    filtered_embeddings_sub_topics_df[\"publication_date\"].tolist(),\n",
        "    nr_bins=20\n",
        "    )\n",
        "\n",
        "# Save the plot\n",
        "subtopics_over_time.to_html(\"/content/drive/My Drive/subtopics_over_time.html\")\n",
        "\n",
        "# Show ad-topics related over time\n",
        "subtopic_model.visualize_topics_over_time(\n",
        "    subtopics_over_time\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLUL9u0PE99T"
      },
      "source": [
        "Now that we have a more granular look at our big topic that contains both privacy and tracking topics, we can dig deeper and get them to have our anchor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LWXchNZFMPI"
      },
      "outputs": [],
      "source": [
        "similar_subtopics_tracking, similarity_sub_tracking = subtopic_model.find_topics(\"tracking\", top_n=1)\n",
        "similar_subtopics_privacy, similarity_sub_privacy = subtopic_model.find_topics(\"data privacy\", top_n=1)\n",
        "\n",
        "print(f'tracking topic : {similar_subtopics_tracking} %:{similarity_sub_tracking[0]}')\n",
        "print(f'privacy topic : {similar_subtopics_privacy} %:{similarity_sub_privacy[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG4WGZRFR6Xl"
      },
      "source": [
        "This shows what we were expecting. As we said before, models based on the distributional hypothesis are not very good at this because antonyms often appear in similar contexts. So the submodel tells me that the same topic is at the same time the most similar to opposite words.\n",
        "\n",
        "To counter that, it has been decided to find manually a topic that matches the most our \"tracking\" topic. We now have our privacy topic (173 docs) and our tracking topic (750 docs). The latter got chosen because of the main focus of the documents which are network analysis that corresponds the most, according to me, to the definition of tracking techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUNzXdmcaodM"
      },
      "outputs": [],
      "source": [
        "# get all topics (int) and add them to every documents\n",
        "filtered_embeddings_sub_topics_df[\"topics\"] = subtopic_model.topics_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzJkbFhr-wTx"
      },
      "outputs": [],
      "source": [
        "subtopic_model.get_topic_info(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l_EW3Tj-3vR"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic_info(11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x79alwvnGOvY"
      },
      "source": [
        "### Compute polarity score between topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vM2xnZYG1EZ"
      },
      "source": [
        "How does a polarity score works ? It is calculated using (positive- negative) / (positive + negative). This result in a score ranging from -1 to +1. We can then have a better understanding of the topics close to those in privacy topics and those in tracking topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JMwBHQkx1FS"
      },
      "outputs": [],
      "source": [
        "# Get privacy topic embeddings\n",
        "privacy_centroid = subtopic_model.topic_embeddings_[12]\n",
        "\n",
        "# Get tracking topic embeddings\n",
        "tracking_centroid = topic_model.topic_embeddings_[11]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FA4ifAnfXQV"
      },
      "outputs": [],
      "source": [
        "# Get topic embeddings\n",
        "topic_vectors = topic_model.topic_embeddings_  # Shape: (n_topics, embedding_dim)\n",
        "topic_ids = topic_model.get_topic_info().Topic.tolist()  # Topic numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxYbTY__20Qf"
      },
      "outputs": [],
      "source": [
        "# Calculate the polarity score (using (positive - negative) / (positive + negative))\n",
        "def polarity_score_normalized(topic_vector, priv_centroid, track_centroid):\n",
        "  sim_privacy = cosine_similarity([topic_vector], [priv_centroid])[0][0]\n",
        "  sim_tracking = cosine_similarity([topic_vector], [track_centroid])[0][0]\n",
        "\n",
        "  return (sim_privacy - sim_tracking) / (sim_privacy + sim_tracking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh7oofm4jAMN"
      },
      "outputs": [],
      "source": [
        "scores = {}\n",
        "for topic_id, topic_vector in zip(topic_ids, topic_vectors):\n",
        "  score = polarity_score_normalized(\n",
        "      topic_vector,\n",
        "      privacy_centroid,\n",
        "      tracking_centroid\n",
        "      )\n",
        "  scores[topic_id] = score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b8KlcwwAA0T"
      },
      "outputs": [],
      "source": [
        "# Convert to Series and sort\n",
        "corrplot = pd.Series(scores).sort_values()\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(17, 12))\n",
        "\n",
        "# TwoSlopeNorm: midpoint = 0 (neutral), red = negative, green = positive\n",
        "vmin = min(scores.values())\n",
        "vmax = max(scores.values())\n",
        "norm = TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
        "colors = [plt.cm.RdYlGn(norm(c)) for c in corrplot.values]\n",
        "\n",
        "# Horizontal bar plot\n",
        "corrplot.plot.barh(color=colors, ax=ax)\n",
        "\n",
        "# Labeling\n",
        "ax.set_xlabel(\"Polarity Score (Privacy vs Tracking)\")\n",
        "ax.set_ylabel(\"Topic ID\")\n",
        "ax.set_title(\"Topic Alignment with Privacy (Red = Tracking-Aligned, Green = Privacy-Aligned)\")\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZRNBdMAHhcb"
      },
      "source": [
        "This plot shows the problem that we have with embeddings. For 2 opposites words, we end up having a really tight polarity score indicating small difference between the 2 topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8zcMez6WyF0"
      },
      "source": [
        "### Try with standard techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_kW-q8SXz2R"
      },
      "source": [
        "Since we have some troubles with the embeddings, we're going to use a technique that do not requires embeddings. We are going with traditionnal machine learning model like logistic regression, SVM and others.\n",
        "\n",
        "By doing this, we are going to classify our patents from 0 to 2 depending if they are tracking related (0), neutrals (1) or privacy related (2).\n",
        "\n",
        "It is interesting to note that other tools have been tried like Vader, a sentiment analyser classifier based on a lexicon, and an NLI (Natural Language Inference) based on embeddings which lead to the same problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95FyZcA3fix6"
      },
      "source": [
        "#### Create train dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s-DYh1agNBp"
      },
      "source": [
        "Here is the method for spliting the dataset and obtain a few samples in order to manually classify them. A focus has been made on the privacy-related topic and tracking-related-topic (the latter defined arbitrarely).\n",
        "\n",
        "One thing interesting to note is the way the classification has been made, or why some patents were classify (manually) as privacy-related or tracking-related.\n",
        "\n",
        "- If the purpose of the patent is about privacy or tracking, they were classify according to the purpose.\n",
        "- If they were about data processing but not related to any of them, they were classify as neutral.\n",
        "- Some patents may include tracking but the purpose was the development of a privacy tool, then they have been categorized as privacy ; which is also the reason why they are so close together.\n",
        "- Some patents may have words like \"location\" which may think of tracking but are actually related on the location of fingers on a screen.\n",
        "- If patents shows techniques to visualize behaviors such as trend data, this is also categorize as tracking.\n",
        "\n",
        "This training dataset has been manually labelled and consists of :\n",
        "\n",
        "| Label| Name       | Amount|\n",
        "| :--- | :------:   | ----: |\n",
        "| 0    |   Tracking | 210   |\n",
        "| 1    |   Neutral  | 655   |\n",
        "| 2    |  Privacy   | 166   |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlGeEMJhjiVS"
      },
      "outputs": [],
      "source": [
        "# Import training set\n",
        "train_df = pd.read_excel('/content/drive/My Drive/train_df.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yn1CUL2f-sD"
      },
      "source": [
        "#### Split the train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyKKZ3d5rgZb"
      },
      "outputs": [],
      "source": [
        "# Instanciate vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = vectorizer.fit_transform(train_df['processing'])\n",
        "y = train_df['labels']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the models"
      ],
      "metadata": {
        "id": "PkFUX_H_m4WP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaPkxqFScB_S"
      },
      "outputs": [],
      "source": [
        "def fit_best_model(model, params) :\n",
        "\n",
        "    scoring = {\n",
        "        'f1': make_scorer(f1_score, average='macro'),\n",
        "        'precision': make_scorer(precision_score, average='macro'),\n",
        "        'recall': make_scorer(recall_score, average='macro')\n",
        "    }\n",
        "\n",
        "    # Fit using gridSearchCV for cross-validation\n",
        "    model_cv = GridSearchCV(\n",
        "      model,\n",
        "      params,\n",
        "      cv=5,\n",
        "      n_jobs=-1, # The number of CPU cores used during cv loop. -1 means all\n",
        "      return_train_score=True,\n",
        "      verbose = 0, # Enable verbose output, 0 = silent\n",
        "      # P quantifies how many of the positive predictions made by the model were actually correct\n",
        "      # R measures how well the model identifies all the actual positive instances\n",
        "      # f1 balance the 2 providing a single value that represents the overall performance\n",
        "      scoring = scoring,\n",
        "      refit='f1'\n",
        "    )\n",
        "\n",
        "    model_cv.fit(X, y)\n",
        "\n",
        "    # Return the best parameters giving the best results\n",
        "    # the best estimator that was chosen by the search,\n",
        "    # the scorer function used on the held out data to choose the best parameters for the model.\n",
        "    return model_cv.best_params_, model_cv.best_estimator_, model_cv.cv_results_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAbVi9YoW61e"
      },
      "outputs": [],
      "source": [
        "# Logistic regression\n",
        "params_lr = {\n",
        "    'fit_intercept' : [True, False],\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'dual' : [False], # Dual or primal formulation\n",
        "    'penalty' : ['l2'], # l1 and elasticnet not always supported with multiclass\n",
        "    'solver' : ['lbfgs', 'newton-cg', 'newton-cholesky', 'sag', 'saga'], # only classes that supports multiclass\n",
        "    'tol' : [1e-4, 1e-3, 1e-2], # Tolerance for stopping criteria\n",
        "    'max_iter' : [100, 500, 1000] , # Max iteration during training\n",
        "    'class_weight' : ['balanced'] # Give more weight to some classes, if None all classes are 1\n",
        "}\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "lr_best_params_, lr_best_estimator_, lr_results_ = fit_best_model(lr, params_lr)\n",
        "# Save the model\n",
        "joblib.dump(lr_best_estimator_, '/content/drive/My Drive/lr_best_estimator_.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr4ZfKYsXhDH"
      },
      "outputs": [],
      "source": [
        "# Support Vector Machine (SVM)\n",
        "params_lsvc = {\n",
        "    'penalty' : ['l2'], # Norm used in penalization\n",
        "    'loss' : ['hinge', 'squared_hinge'], # Specifies loss function\n",
        "    'dual' : [True, False], # Select algo to either solve dual or primal optimization problem\n",
        "    'tol' : [1e-4, 1e-3, 1e-2], # Tolerance for stopping criteria\n",
        "    'C' : [0.001, 0.01, 0.1, 1, 10, 100], # Regularization parameter\n",
        "    'multi_class' : ['ovr', 'crammer_singer'], # Determines multi-class strategy. Crammer-singer rarely leads to better accuracy so ovr is chosen\n",
        "    'fit_intercept' : [True, False], # Wether or not to fit an intercept\n",
        "    'intercept_scaling' : [0.1, 1, 10],\n",
        "    'class_weight' : ['balanced'], # Give more weight to some classes, if None all classes are 1\n",
        "    'random_state' : [42], # Control the randomness of the output, 42 is the least possible\n",
        "    'max_iter' : [100, 500, 1000] , # Max iteration during training\n",
        "}\n",
        "\n",
        "lsvc = LinearSVC()\n",
        "\n",
        "# Train the model\n",
        "lsvc_best_params, lsvc_best_estimator, lsvc_results_ = fit_best_model(lsvc, params_lsvc)\n",
        "# Save the model\n",
        "joblib.dump(lsvc_best_estimator, '/content/drive/My Drive/lsvc_best_estimator_.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcrw9wM5XlgU"
      },
      "outputs": [],
      "source": [
        "# Random forest\n",
        "params_rfc = {\n",
        "    'n_estimators' : [100, 300, 500], # Number of trees in the forest\n",
        "    'criterion' : ['gini'], # Measure to split a node\n",
        "    'max_depth' : [None, 30, 70], # Max depth of the tree\n",
        "    'min_samples_split' : [2, 5, 10], # Min number of samples required to split an internal node\n",
        "    'min_samples_leaf' : [1, 2, 4], # Min number of samples required to be at a leaf node\n",
        "    'max_features' : ['sqrt', 'log2', None], # Number of features to consider when looking for the best split\n",
        "    'random_state' : [42], # Control the randomness of the output, 42 is the least possible\n",
        "    'class_weight' : ['balanced'],  # Give more weight to some classes, if None all classes are 1\n",
        "}\n",
        "\n",
        "rfc = RandomForestClassifier()\n",
        "\n",
        "# Train the model\n",
        "rfc_best_params, rfc_best_estimator, rfc_results_ = fit_best_model(rfc, params_rfc)\n",
        "# Save the model\n",
        "joblib.dump(rfc_best_estimator, '/content/drive/My Drive/rfc_best_estimator_.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf5JmhVXYsyH"
      },
      "outputs": [],
      "source": [
        "# XGBoost\n",
        "# Compute scale_pos_weight as ratio of negative to positive instances in train set\n",
        "# according to https://xgboosting.com/xgboost-scale_pos_weight-vs-sample_weight-for-imbalanced-classification/\n",
        "scale_pos_weight = 197 / 166  # ≈ 1.187\n",
        "\n",
        "params_xgb = {\n",
        "    'objective': ['multi:softmax', 'multi:softprob'], # Defining multi-classification with softmax and softprob\n",
        "    'num_class': [3], # Number of classes\n",
        "    'max_depth': [3, 5, 10], # Max depth of a tree 4-6\n",
        "    'gamma': [0, 0.5, 1], # Min loss reduction required to make a further partition on a leaf node of the tree. Larger gamma, the more conservative the algo is\n",
        "    'n_estimators': [100, 300, 500],\n",
        "    'scale_pos_weight': [scale_pos_weight], # Control balance of pos and neg weights, useful for unbalanced classes\n",
        "    'eval_metric': ['mlogloss'] # Metric used for model assessment\n",
        "}\n",
        "\n",
        "xgbc = xgb.XGBClassifier()\n",
        "\n",
        "# Train the model\n",
        "xgbc_best_params, xgbc_best_estimator, xgbc_results_ = fit_best_model(xgbc, params_xgb)\n",
        "# Save the model\n",
        "joblib.dump(xgbc_best_estimator, '/content/drive/My Drive/xgbc_best_estimator_.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWDIwwePbFmI"
      },
      "outputs": [],
      "source": [
        "# Load the model (if already trained)\n",
        "model_lr = joblib.load('/content/drive/My Drive/lr_best_estimator_.pkl')\n",
        "model_lsvc = joblib.load('/content/drive/My Drive/lsvc_best_estimator_.pkl')\n",
        "model_rfc = joblib.load('/content/drive/My Drive/rfc_best_estimator_.pkl')\n",
        "model_xgbc = joblib.load('/content/drive/My Drive/xgbc_best_estimator_.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_sWCwTsKRCb"
      },
      "source": [
        "### Assess the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOl0pslKKpUL"
      },
      "source": [
        "Now that all of our models are tested, we can find the one that gives the \"best result\". This also means that we take one that ... (talk about false positive and true negative and why we chose the one that makes more true negative or false positive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D03qyCQ9DAJ"
      },
      "outputs": [],
      "source": [
        "# Logistic regression assessment\n",
        "print(\"Mean precision:\", lr_results_['mean_test_precision'])\n",
        "print(\"Mean recall:\", lr_results_['mean_test_recall'])\n",
        "print(\"Mean f1:\", lr_results_['mean_test_f1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnTum6skBEh4"
      },
      "outputs": [],
      "source": [
        "# Support Vector Machine (SVM) assessment\n",
        "print(\"Mean precision:\", lsvc_results_['mean_test_precision'])\n",
        "print(\"Mean recall:\", lsvc_results_['mean_test_recall'])\n",
        "print(\"Mean f1:\", lsvc_results_['mean_test_f1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O9cWBFzBKqm"
      },
      "outputs": [],
      "source": [
        "# Random forest assessment\n",
        "print(\"Mean precision:\", rfc_results_['mean_test_precision'])\n",
        "print(\"Mean recall:\", rfc_results_['mean_test_recall'])\n",
        "print(\"Mean f1:\", rfc_results_['mean_test_f1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX3pYCuzAscX"
      },
      "outputs": [],
      "source": [
        "# XGBoost assessment\n",
        "print(\"Mean precision:\", xgbc_results_['mean_test_precision'])\n",
        "print(\"Mean recall:\", xgbc_results_['mean_test_recall'])\n",
        "print(\"Mean f1:\", xgbc_results_['mean_test_f1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After considering these 4 algorithms, training them and looking at the results, it has been decided to keep going with the Logistic Regression algorithm. This choice was made because of the high and stable scores of the predictions. There is a high mean prediction (around 0.75) while a good mean recall (around 0.75) which gives a good balance with the mean F1 score (around 0.75). It doesn't show overfitting like the decision tree was giving with, sometimes, a mean precision of more than 0.80 but a mean recall of 0.60."
      ],
      "metadata": {
        "id": "dsjai22AeBjn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPxNtwakJ7oY"
      },
      "source": [
        "### Predict the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dataset without training\n",
        "predict_df = pd.read_excel('/content/drive/My Drive/predict_df.xlsx')"
      ],
      "metadata": {
        "id": "2zq4Lv6OpvVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpWffxlPhsU3"
      },
      "outputs": [],
      "source": [
        "# Get lines to predict\n",
        "documents = predict_df['processing']\n",
        "\n",
        "# Vectorize documents with tfidf\n",
        "X_tfidf = vectorizer.fit_transform(documents)  # sparse matrix (n_samples, n_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_6XcBVZJ_VH"
      },
      "outputs": [],
      "source": [
        "# Make the prediction over the dataset\n",
        "predicted_results = model_lr.predict(X_tfidf)\n",
        "\n",
        "# Assign predicted results made by the model to new column\n",
        "predict_df['predicted_labels'] = predicted_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign same column to train_df\n",
        "train_df[\"predicted_labels\"] = train_df[\"labels\"]\n",
        "\n",
        "# Merge 2 datasets\n",
        "patents_df = pd.concat([train_df, predict_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "dP2pKDQDrFpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbijYP0IJYBk"
      },
      "source": [
        "### Display evolution of privacy, neutral and tracking related topics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by looking at the overall evolution of the privacy and non privacy related patents."
      ],
      "metadata": {
        "id": "pcqSDLrhibmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add publication_year\n",
        "patents_df[\"publication_year\"] = patents_df[\"publication_date\"].dt.year\n",
        "\n",
        "# Group by year and topic\n",
        "count_df = patents_df.groupby(\n",
        "    [\"publication_year\", \"predicted_labels\"]\n",
        ").size().reset_index(name=\"count\")\n",
        "\n",
        "# Plot with color = topics\n",
        "plot_topic_by_year = px.bar(\n",
        "    count_df,\n",
        "    x=\"publication_year\",\n",
        "    y=\"count\",\n",
        "    color=\"predicted_labels\",\n",
        "    title=\"Number of patents per year by Topic\",\n",
        "    barmode=\"stack\"\n",
        ")\n",
        "\n",
        "# Save as HTML\n",
        "plot_topic_by_year.write_html(\"/content/drive/My Drive/patents_by_privacy_chart.html\")\n",
        "\n",
        "# Show the chart\n",
        "plot_topic_by_year.show()"
      ],
      "metadata": {
        "id": "1y8Eay2oiW0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot the evolution"
      ],
      "metadata": {
        "id": "NVWduBPPpHwD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDaa_OfvJyMf"
      },
      "outputs": [],
      "source": [
        "# Count total patents per year\n",
        "total_per_year = patents_df.groupby('publication_year').size().rename('total')\n",
        "# Count tracking and privacy patents per year\n",
        "tracking_counts = patents_df[patents_df['predicted_labels'] == 0].groupby('publication_year').size().rename('tracking')\n",
        "privacy_counts = patents_df[patents_df['predicted_labels'] == 2].groupby('publication_year').size().rename('privacy')\n",
        "\n",
        "# Merge into one DataFrame\n",
        "yearly_stats = pd.concat([total_per_year, tracking_counts, privacy_counts], axis=1).fillna(0)\n",
        "yearly_stats['tracking_prop'] = yearly_stats['tracking'] / yearly_stats['total']\n",
        "yearly_stats['privacy_prop'] = yearly_stats['privacy'] / yearly_stats['total']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(yearly_stats.index, yearly_stats['tracking_prop'], marker='o', label='Tracking')\n",
        "plt.plot(yearly_stats.index, yearly_stats['privacy_prop'], marker='s', label='Privacy')\n",
        "\n",
        "plt.axvline(x=2020, color='red', linestyle='--', label='Cookie Phase-Out (2020)')\n",
        "plt.title('Proportion of Tracking and Privacy-Related Patents Over Time')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Proportion')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('/content/drive/MyDrive/patent_proportions_plot.png')\n",
        "\n",
        "# Display plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pqp1yQ18kM40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we see that privacy and tracking related patents represent a small amount of patents. Like said before, other patents are related to technological improvements in self driving cars, machine learning models and so on.\n",
        "\n",
        "However, between 2014 and 2024, there's a significant drop in the number of tracking-related patents. On the other hand, in the same period, privacy-related patents grew.\n",
        "\n",
        "Finally, the inflection point, around 2019, can show a strategic shift in Google's strategy regarding the rise of privacy scrutinity, regulatory changes (GDPR, etc.), and so on."
      ],
      "metadata": {
        "id": "-B0SEssNpbzC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq_vzXrV-VUs"
      },
      "source": [
        "## Statistical test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4vfCziJDtRo"
      },
      "outputs": [],
      "source": [
        "# Statistical test (chi squared test of independence)\n",
        "# Create a column for period\n",
        "patents_df['period'] = patents_df['publication_year'].apply(lambda x: 'pre_2020' if x < 2020 else 'post_2020')\n",
        "\n",
        "# Create contingency table\n",
        "contingency = pd.crosstab(patents_df['period'], patents_df['predicted_labels'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chi2, p, dof, expected = chi2_contingency(contingency)\n",
        "\n",
        "print(f\"Chi-squared statistic: {chi2:.3f}\")\n",
        "print(f\"Degrees of freedom: {dof}\")\n",
        "print(f\"P-value: {p:.4f}\")\n",
        "\n",
        "if p < 0.05:\n",
        "    print(\"Significant change in distribution after 2020\")\n",
        "else:\n",
        "    print(\"No statistically significant change in distribution\")"
      ],
      "metadata": {
        "id": "_AgCis0el14o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A 60.1 result is a significant change and confirms there's a non-random change in patent distribution after 2020, aligning well with known external pressures on data practices."
      ],
      "metadata": {
        "id": "Jej1t2DzCO5y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AcEdg1ahnYy"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owtZkrxphqeZ"
      },
      "source": [
        "This study analyzed Google’s patent activity from 2010 to 2025 to investigate shifts in technological focus in response to the phasing out of third-party cookies. By applying topic modeling (BERTopic) combined with document embeddings and statistical analysis, we identified a significant transition in the nature of Google’s patents related to user data handling.\n",
        "\n",
        "The results show a steady decline in tracking-related patents, dropping from 3% to 0.5% over 15 years, with a marked inflection point around 2018–2019, where privacy-related patents began to outnumber tracking ones. This shift coincides with increasing regulatory pressure (e.g., GDPR, CCPA) and the public announcement of Google's plans to eliminate third-party cookies.\n",
        "\n",
        "A chi-squared test confirmed that this trend is not random, revealing a statistically significant change in patent distribution after 2020 (χ² = 60.993, p < 0.001). This suggests a deliberate strategic pivot.\n",
        "\n",
        "These findings support the rejection of the null hypothesis (H₀). There is strong evidence that Google’s increased activity in privacy-related patent filings reflects the development of alternative targeting systems, aligning with the company’s stated privacy initiatives — while possibly preserving, or even enhancing, its capabilities in user profiling through new technical paradigms."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limitations and considerations"
      ],
      "metadata": {
        "id": "joLJy-9ZFY3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this study offers compelling evidence of a thematic shift in Google’s patent filings toward privacy-related technologies, several constraints limit the scope and interpretation of the findings:\n",
        "\n",
        "- Patents ≠ Products: Not all patents lead to actual deployment. Many are defensive, strategic, or speculative in nature. Therefore, the presence of privacy-related patents does not guarantee implementation in Google’s real-world advertising or tracking systems.\n",
        "\n",
        "- Interpretation of Patent Language: Patent abstracts and claims often use generalized, technical, or vague language. This introduces ambiguity when classifying them under themes like \"privacy\" or \"tracking,\" even with semantic clustering models.\n",
        "\n",
        "- Algorithmic Limitations: Techniques such as BERTopic rely on vector embeddings that can misrepresent nuanced legal or contextual meanings, especially in fields with evolving terminologies like privacy and surveillance."
      ],
      "metadata": {
        "id": "Hx6Dk4UWFdW_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}